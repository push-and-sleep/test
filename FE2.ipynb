{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import joblib\n",
    "import datetime\n",
    "import itertools\n",
    "import os.path as path\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "font_dirs = ['/home/workspace/user-workspace/font']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    "    \n",
    "plt.rcParams['font.family'] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/workspace/user-workspace/slim_train.parquet'\n",
    "test_path ='/home/workspace/user-workspace/slim_test.parquet'\n",
    "encoder = '/home/workspace/user-workspace/cat_encoder.json'\n",
    "decoder = '/home/workspace/user-workspace/inverse_cat_encoder.json'\n",
    "data_dir = '/home/workspace/user-workspace/junheon/data/task150/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_ratio = 1\n",
    "bagging_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(train_path).reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet(test_path).reset_index().rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TRD_NO</th>\n",
       "      <th>REQ_DD</th>\n",
       "      <th>CP_CD</th>\n",
       "      <th>CP_NM</th>\n",
       "      <th>GODS_NM</th>\n",
       "      <th>PAYR_SEQ</th>\n",
       "      <th>MPHN_NO</th>\n",
       "      <th>COMMC_CLF</th>\n",
       "      <th>AC_PAY_AMT</th>\n",
       "      <th>...</th>\n",
       "      <th>MAX_NPAY_CNT_24M</th>\n",
       "      <th>TRD_CNT_6M</th>\n",
       "      <th>REAL_TRD_CNT_6M</th>\n",
       "      <th>NIGHT_TRD_RT_6M</th>\n",
       "      <th>AVG_AMT_6M</th>\n",
       "      <th>MAX_LMT_3M_RT</th>\n",
       "      <th>NPAY_CNT_24M</th>\n",
       "      <th>NPAY_CNT_12MNTS</th>\n",
       "      <th>NPAY_AMT_60M</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9iSJiZ+6F/ojR81Swb0CU5oBNWIuJuSsmXsb7aPoWro=</td>\n",
       "      <td>20190701</td>\n",
       "      <td>270</td>\n",
       "      <td>308</td>\n",
       "      <td>394324</td>\n",
       "      <td>945077</td>\n",
       "      <td>940112</td>\n",
       "      <td>0</td>\n",
       "      <td>49900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>111716.0</td>\n",
       "      <td>0.49900</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>UStyI9p3TkoNswhaARv+Dzznl7NW4o49XlSEv/jy3/U=</td>\n",
       "      <td>20190701</td>\n",
       "      <td>837</td>\n",
       "      <td>173</td>\n",
       "      <td>735708</td>\n",
       "      <td>1850868</td>\n",
       "      <td>919223</td>\n",
       "      <td>1</td>\n",
       "      <td>14300</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>30776.0</td>\n",
       "      <td>0.21767</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PgOykMEKX3so4zIpsNcU+zNt+Nj4VQdwjDB+NlVIJN4=</td>\n",
       "      <td>20190702</td>\n",
       "      <td>785</td>\n",
       "      <td>539</td>\n",
       "      <td>499519</td>\n",
       "      <td>415916</td>\n",
       "      <td>71968</td>\n",
       "      <td>2</td>\n",
       "      <td>440000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>177392.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9ngELbCK8cqbPY53oe0eUY+tPvTR/OU7KIMg0pDdh4w=</td>\n",
       "      <td>20190701</td>\n",
       "      <td>1198</td>\n",
       "      <td>7</td>\n",
       "      <td>272349</td>\n",
       "      <td>2477403</td>\n",
       "      <td>568377</td>\n",
       "      <td>1</td>\n",
       "      <td>9907</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>123318.0</td>\n",
       "      <td>0.52973</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sT/zlLzq7AK9QkTSH51L5+LSNY+zkfwrr7PlR4EOMtI=</td>\n",
       "      <td>20190702</td>\n",
       "      <td>1198</td>\n",
       "      <td>7</td>\n",
       "      <td>674245</td>\n",
       "      <td>3452051</td>\n",
       "      <td>190416</td>\n",
       "      <td>0</td>\n",
       "      <td>47600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>90333.0</td>\n",
       "      <td>0.33100</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        TRD_NO    REQ_DD  CP_CD  CP_NM  \\\n",
       "0   0  9iSJiZ+6F/ojR81Swb0CU5oBNWIuJuSsmXsb7aPoWro=  20190701    270    308   \n",
       "1   1  UStyI9p3TkoNswhaARv+Dzznl7NW4o49XlSEv/jy3/U=  20190701    837    173   \n",
       "2   2  PgOykMEKX3so4zIpsNcU+zNt+Nj4VQdwjDB+NlVIJN4=  20190702    785    539   \n",
       "3   3  9ngELbCK8cqbPY53oe0eUY+tPvTR/OU7KIMg0pDdh4w=  20190701   1198      7   \n",
       "4   4  sT/zlLzq7AK9QkTSH51L5+LSNY+zkfwrr7PlR4EOMtI=  20190702   1198      7   \n",
       "\n",
       "   GODS_NM  PAYR_SEQ  MPHN_NO  COMMC_CLF  AC_PAY_AMT  ...  MAX_NPAY_CNT_24M  \\\n",
       "0   394324    945077   940112          0       49900  ...                 0   \n",
       "1   735708   1850868   919223          1       14300  ...                 0   \n",
       "2   499519    415916    71968          2      440000  ...                 0   \n",
       "3   272349   2477403   568377          1        9907  ...                 0   \n",
       "4   674245   3452051   190416          0       47600  ...                 0   \n",
       "\n",
       "   TRD_CNT_6M  REAL_TRD_CNT_6M  NIGHT_TRD_RT_6M  AVG_AMT_6M  MAX_LMT_3M_RT  \\\n",
       "0           5                1           0.6429    111716.0        0.49900   \n",
       "1           5                5           0.4444     30776.0        0.21767   \n",
       "2           6                0           0.1111    177392.0        1.00000   \n",
       "3           6                6           0.0645    123318.0        0.52973   \n",
       "4           3                3           0.1429     90333.0        0.33100   \n",
       "\n",
       "   NPAY_CNT_24M  NPAY_CNT_12MNTS  NPAY_AMT_60M  target  \n",
       "0             1                7           0.0       0  \n",
       "1             0                1           0.0       0  \n",
       "2             0                2           0.0       0  \n",
       "3             0                0           0.0       0  \n",
       "4             1                5           0.0       0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-09 09:31:19.311746\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = train_df[train_df['target']==1]\n",
    "negative = train_df[train_df['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274713\n",
      "7591835\n"
     ]
    }
   ],
   "source": [
    "print(len(positive))\n",
    "print(len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    negative_sample = negative.sample(n=(len(positive)*negative_ratio), random_state=seed)\n",
    "    id_list = positive.id.tolist() + negative_sample.id.tolist()\n",
    "    joblib.dump(id_list, f\"{data_dir}indices_{seed}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feture Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-07 14:17:35.867743\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"basic_feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = {\n",
    "    \"id\": \"uint32\",\n",
    "    \"AC_PAY_AMT\": \"uint32\",\n",
    "    \"AGE\": \"uint8\",\n",
    "    \"SMS_RE_SND_CNT\": \"uint8\",\n",
    "    \"ACUM_RCPT_AMT\": \"int32\",\n",
    "    \"MAX_NPAY_CNT_24M\": \"uint8\",\n",
    "    \"TRD_CNT_6M\": \"uint8\",\n",
    "    \"REAL_TRD_CNT_6M\": \"uint8\",\n",
    "    \"NPAY_CNT_24M\": \"uint8\",\n",
    "    \"NPAY_CNT_12MNTS\": \"uint8\",\n",
    "    \"MM_LMT_AMT\": \"float32\",\n",
    "    \"REMD_LMT_AMT\": \"float32\",\n",
    "    \"NPAY_AMT_24M\": \"float32\",\n",
    "    \"NIGHT_TRD_RT_6M\": \"float32\",\n",
    "    \"AVG_AMT_6M\": \"float32\",\n",
    "    \"MAX_LMT_3M_RT\": \"float32\",\n",
    "    \"NPAY_AMT_60M\": \"float32\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features = [\n",
    "    \"COMMC_CLF\", \"NPAY_YN\", \"PAY_MTHD_CD\", \"ARS_AUTHTI_YN\", \"GNDR\", \"FOREI_YN\",  \"AUTHTI_CLF_FLG\", \n",
    "    \"SVC_CLF_NM\", \"CP_M_CLF_NM\", \"CP_S_CLF_NM\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[list(basic_features.keys()) + category_features + [\"target\"]].astype(basic_features)\n",
    "df_test = test_df[list(basic_features.keys()) + category_features].astype(basic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['NPAY_YN'] = [-99 if x==2 else x for x in df_train['NPAY_YN']]\n",
    "df_test['NPAY_YN'] = [-99 if x==2 else x for x in df_test['NPAY_YN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['PAY_MTHD_CD'] = [-99 if x==0 else x for x in df_train['PAY_MTHD_CD']]\n",
    "df_test['PAY_MTHD_CD'] = [-99 if x==0 else x for x in df_test['PAY_MTHD_CD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['MM_LMT_AMT'] = [1000000 if np.isnan(x) else x for x in df_train['MM_LMT_AMT']]\n",
    "df_test['MM_LMT_AMT'] = [1000000 if np.isnan(x) else x for x in df_test['MM_LMT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['REMD_LMT_AMT'] = [1000000 if np.isnan(x) else x for x in df_train['REMD_LMT_AMT']]\n",
    "df_test['REMD_LMT_AMT'] = [1000000 if np.isnan(x) else x for x in df_test['REMD_LMT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ARS_AUTHTI_YN'] = [-99 if x==2 else x for x in df_train['ARS_AUTHTI_YN']]\n",
    "df_test['ARS_AUTHTI_YN'] = [-99 if x==2 else x for x in df_test['ARS_AUTHTI_YN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['CP_M_CLF_NM'] = [-99 if x==5 else x for x in df_train['CP_M_CLF_NM']]\n",
    "df_test['CP_M_CLF_NM'] = [-99 if x==5 else x for x in df_test['CP_M_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['CP_S_CLF_NM'] = [-99 if x==34 else x for x in df_train['CP_S_CLF_NM']]\n",
    "df_test['CP_S_CLF_NM'] = [-99 if x==34 else x for x in df_test['CP_S_CLF_NM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-07 14:18:49.328103\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Cardinality Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-07 14:18:49.337561\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"high_cardi_basic_feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = [\"id\", \"CP_CD\", \"GODS_NM\", \"PAYR_SEQ\", \"PAYR_IP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[basic_features]\n",
    "df_test = test_df[basic_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-07 14:18:54.871364\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## month, day (hour은 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-07 14:18:54.880416\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"month_day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'REQ_DD']].astype({\"REQ_DD\": \"str\"})\n",
    "df_test = test_df[['id', 'REQ_DD']].astype({\"REQ_DD\": \"str\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['month'] = df_train.REQ_DD.astype(\"str\").str[4:6].astype(\"uint8\")\n",
    "df_train['day'] = df_train.REQ_DD.astype(\"str\").str[6:8].astype(\"uint8\")\n",
    "df_test['month'] = df_test.REQ_DD.astype(\"str\").str[4:6].astype(\"uint8\")\n",
    "df_test['day'] = df_test.REQ_DD.astype(\"str\").str[6:8].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'month', 'day']]\n",
    "df_test = df_test[['id', 'month', 'day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-07 14:19:21.921631\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 거래금액 (AC_PAY_AMT) 49000원, 11000원\n",
    "\n",
    "49000원: 롤 rp 충전 최고 금액\n",
    "\n",
    "11000원: 아프리카 별풍선 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-07 14:19:21.926522\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"pay_amt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'AC_PAY_AMT']]\n",
    "df_test = test_df[['id', 'AC_PAY_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-96c0763cc145>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_49900_PAY_AMT\"] = (df_train['AC_PAY_AMT']==49900)\n",
      "<ipython-input-46-96c0763cc145>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_11000_PAY_AMT\"] = (df_train['AC_PAY_AMT']==11000)\n",
      "<ipython-input-46-96c0763cc145>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"is_11000s_PAY_AMT\"] = (df_train['AC_PAY_AMT']%11000==0)\n",
      "<ipython-input-46-96c0763cc145>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_49900_PAY_AMT\"] = (df_test['AC_PAY_AMT']==49900)\n",
      "<ipython-input-46-96c0763cc145>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_11000_PAY_AMT\"] = (df_test['AC_PAY_AMT']==11000)\n",
      "<ipython-input-46-96c0763cc145>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"is_11000s_PAY_AMT\"] = (df_test['AC_PAY_AMT']%11000==0)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"is_49900_PAY_AMT\"] = (df_train['AC_PAY_AMT']==49900)\n",
    "df_train[\"is_11000_PAY_AMT\"] = (df_train['AC_PAY_AMT']==11000)\n",
    "df_train[\"is_11000s_PAY_AMT\"] = (df_train['AC_PAY_AMT']%11000==0)\n",
    "df_test[\"is_49900_PAY_AMT\"] = (df_test['AC_PAY_AMT']==49900)\n",
    "df_test[\"is_11000_PAY_AMT\"] = (df_test['AC_PAY_AMT']==11000)\n",
    "df_test[\"is_11000s_PAY_AMT\"] = (df_test['AC_PAY_AMT']%11000==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_49900_PAY_AMT', 'is_11000_PAY_AMT', 'is_11000s_PAY_AMT']]\n",
    "df_test = df_test[['id', 'is_49900_PAY_AMT', 'is_11000_PAY_AMT', 'is_11000s_PAY_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-07 14:19:26.036267\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849511</th>\n",
       "      <td>3849511</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7782626</th>\n",
       "      <td>7782626</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626310</th>\n",
       "      <td>6626310</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820200</th>\n",
       "      <td>820200</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6934281</th>\n",
       "      <td>6934281</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549426 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  month  day\n",
       "7              7      7    2\n",
       "62            62      7    2\n",
       "64            64      7    1\n",
       "65            65      7    1\n",
       "66            66      7    1\n",
       "...          ...    ...  ...\n",
       "3849511  3849511      8   29\n",
       "7782626  7782626     10   31\n",
       "6626310  6626310     10   12\n",
       "820200    820200      7   10\n",
       "6934281  6934281     10   17\n",
       "\n",
       "[549426 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(f\"{data_dir}month_day_0.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Features\n",
    "\"CP_CD\", \"GODS_NM\", \"PAYR_SEQ\", \"MPHN_NO\", \"PAYR_IP\"\n",
    "\n",
    "LDA는 리소스 부족으로 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_prefix = \"PCA5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"PCA5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_pairs(feature_list):\n",
    "    return [(col1, col2) for col1, col2 in itertools.product(feature_list, repeat=2) if col1!=col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(col1, col2):\n",
    "    document_path = f\"{data_dir}word_list_{col1}_{col2}.pkl\"\n",
    "    if not path.isfile(document_path):\n",
    "        df = train_df[[col1, col2]]\n",
    "        document_list_size = df[col1].max() + 1\n",
    "        documents = [[] for _ in range(document_list_size)]\n",
    "        for document, word in zip(df[col1], df[col2]):\n",
    "            documents[document].append(word)\n",
    "        documents = [' '.join(map(str, words)) for words in documents]\n",
    "        joblib.dump(documents, document_path)\n",
    "        return documents\n",
    "    with open(document_path, 'rb') as f:\n",
    "        documents = joblib.load(f)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_term_matrix(col1, col2):\n",
    "    documents = get_documents(col1, col2)\n",
    "    vectorizer = TfidfVectorizer(min_df=2, dtype=np.float32)\n",
    "    return vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_vector(col1, col2, n_topics):\n",
    "    latent_path = f\"{data_dir}{column_prefix}_{col1}_{col2}.pkl\"\n",
    "    if not path.isfile(latent_path):\n",
    "        document_term_matrix = get_document_term_matrix(col1, col2)\n",
    "        decomposer = TruncatedSVD(n_components=n_topics, random_state=77)\n",
    "        latent_vector = decomposer.fit_transform(document_term_matrix)\n",
    "        joblib.dump(latent_vector, latent_path)\n",
    "        return latent_vector\n",
    "    with open(latent_path, 'rb') as f:\n",
    "        latent_vector = joblib.load(f)\n",
    "    return latent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_df(column_pairs, df, n_topics):\n",
    "    latent_vectors = []\n",
    "    for pair in column_pairs:\n",
    "        latent_vector = get_latent_vector(pair[0], pair[1], n_topics)\n",
    "        latent_vectors.append(latent_vector.astype(np.float32))\n",
    "    n_columns = n_topics * len(column_pairs)\n",
    "    features = np.zeros(shape=(len(df), n_columns), dtype=np.float32)\n",
    "    columns = []\n",
    "    \n",
    "    for i, pair in enumerate(column_pairs):\n",
    "        offset = i * n_topics\n",
    "        for j in range(n_topics):\n",
    "            columns.append(f\"{column_prefix}-{pair[0]}-{pair[1]}-{j}\")\n",
    "        \n",
    "        for j, value in enumerate(df[pair[0]]):\n",
    "            features[j, offset:offset + n_topics] = latent_vectors[i][value]\n",
    "            \n",
    "    result_df = pd.DataFrame(data=features, columns=columns, index=df.id)\n",
    "    \n",
    "    return result_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = [\"id\", \"CP_CD\", \"GODS_NM\", \"PAYR_SEQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_pair = get_column_pairs(basic_features[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_df = get_latent_df(column_pair, train_df[basic_features], 5)\n",
    "latent_df.to_parquet(f\"{data_dir}{FILE_NAME}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = latent_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[['id', 'CP_CD', 'GODS_NM', 'PAYR_SEQ']]\n",
    "for pair in column_pair:\n",
    "    columns = [f\"{column_prefix}-{pair[0]}-{pair[1]}-{i}\" for i in range(5)]\n",
    "    temp_df = latent_df[columns].reset_index()\n",
    "    temp_df[pair[0]] = train_df[pair[0]]\n",
    "    temp_df[pair[1]] = train_df[pair[1]]\n",
    "    temp_df = temp_df.drop_duplicates(subset=[pair[0], pair[1]])\n",
    "    temp_df = temp_df.drop(columns=['id'])\n",
    "    test_df = test_df.merge(temp_df, on=[pair[0], pair[1]], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns=['CP_CD', 'GODS_NM', 'PAYR_SEQ']).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-09 11:18:02.586315\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Features\n",
    "\"CP_CD\", \"GODS_NM\", \"PAYR_SEQ\", \"MPHN_NO\", \"PAYR_IP\"\n",
    "\n",
    "LDA는 리소스 부족으로 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"NMF5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_prefix = \"NMF5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_pairs(feature_list):\n",
    "    return [(col1, col2) for col1, col2 in itertools.product(feature_list, repeat=2) if col1!=col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(col1, col2):\n",
    "    document_path = f\"{data_dir}word_list_{col1}_{col2}.pkl\"\n",
    "    if not path.isfile(document_path):\n",
    "        df = train_df[[col1, col2]]\n",
    "        document_list_size = df[col1].max() + 1\n",
    "        documents = [[] for _ in range(document_list_size)]\n",
    "        for document, word in zip(df[col1], df[col2]):\n",
    "            documents[document].append(word)\n",
    "        documents = [' '.join(map(str, words)) for words in documents]\n",
    "        joblib.dump(documents, document_path)\n",
    "        return documents\n",
    "    with open(document_path, 'rb') as f:\n",
    "        documents = joblib.load(f)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_term_matrix(col1, col2):\n",
    "    documents = get_documents(col1, col2)\n",
    "    vectorizer = TfidfVectorizer(min_df=2, dtype=np.float32)\n",
    "    return vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_vector(col1, col2, n_topics):\n",
    "    latent_path = f\"{data_dir}{column_prefix}_{col1}_{col2}.pkl\"\n",
    "    if not path.isfile(latent_path):\n",
    "        document_term_matrix = get_document_term_matrix(col1, col2)\n",
    "        decomposer = NMF(n_components=n_topics, random_state=77)\n",
    "        latent_vector = decomposer.fit_transform(document_term_matrix)\n",
    "        joblib.dump(latent_vector, latent_path)\n",
    "        return latent_vector\n",
    "    with open(latent_path, 'rb') as f:\n",
    "        latent_vector = joblib.load(f)\n",
    "    return latent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_df(column_pairs, df, n_topics):\n",
    "    latent_vectors = []\n",
    "    for pair in column_pairs:\n",
    "        latent_vector = get_latent_vector(pair[0], pair[1], n_topics)\n",
    "        latent_vectors.append(latent_vector.astype(np.float32))\n",
    "    n_columns = n_topics * len(column_pairs)\n",
    "    features = np.zeros(shape=(len(df), n_columns), dtype=np.float32)\n",
    "    columns = []\n",
    "    \n",
    "    for i, pair in enumerate(column_pairs):\n",
    "        offset = i * n_topics\n",
    "        for j in range(n_topics):\n",
    "            columns.append(f\"{column_prefix}-{pair[0]}-{pair[1]}-{j}\")\n",
    "        \n",
    "        for j, value in enumerate(df[pair[0]]):\n",
    "            features[j, offset:offset + n_topics] = latent_vectors[i][value]\n",
    "            \n",
    "    result_df = pd.DataFrame(data=features, columns=columns, index=df.id)\n",
    "    \n",
    "    return result_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = [\"id\", \"CP_CD\", \"GODS_NM\", \"PAYR_SEQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_pair = get_column_pairs(basic_features[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_df = get_latent_df(column_pair, train_df[basic_features], 5)\n",
    "latent_df.to_parquet(f\"{data_dir}{FILE_NAME}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = latent_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[['id', 'CP_CD', 'GODS_NM', 'PAYR_SEQ']]\n",
    "for pair in column_pair:\n",
    "    columns = [f\"{column_prefix}-{pair[0]}-{pair[1]}-{i}\" for i in range(5)]\n",
    "    temp_df = latent_df[columns].reset_index()\n",
    "    temp_df[pair[0]] = train_df[pair[0]]\n",
    "    temp_df[pair[1]] = train_df[pair[1]]\n",
    "    temp_df = temp_df.drop_duplicates(subset=[pair[0], pair[1]])\n",
    "    temp_df = temp_df.drop(columns=['id'])\n",
    "    test_df = test_df.merge(temp_df, on=[pair[0], pair[1]], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns=['CP_CD', 'GODS_NM', 'PAYR_SEQ']).to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-07 14:18:49.328103\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age 구간 \n",
    "\n",
    "categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-09 11:20:30.512780\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"age_bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'AGE']]\n",
    "df_test = test_df[['id', 'AGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-d41cb0d64370>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['AGE_bin'] = [0 if x<25 else x for x in df_train['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['AGE_bin'] = [1 if (x>=25 & x<30) else x for x in df_train['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['AGE_bin'] = [2 if (x>=30 & x<35) else x for x in df_train['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['AGE_bin'] = [3 if (x>=35 & x<40) else x for x in df_train['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['AGE_bin'] = [4 if (x>=40 & x<50) else x for x in df_train['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['AGE_bin'] = [5 if (x>=50 & x<60) else x for x in df_train['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['AGE_bin'] = [6 if x>=60 else x for x in df_train['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['AGE_bin'] = [0 if x<25 else x for x in df_test['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['AGE_bin'] = [1 if (x>=25 & x<30) else x for x in df_test['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['AGE_bin'] = [2 if (x>=30 & x<35) else x for x in df_test['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['AGE_bin'] = [3 if (x>=35 & x<40) else x for x in df_test['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['AGE_bin'] = [4 if (x>=40 & x<50) else x for x in df_test['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['AGE_bin'] = [5 if (x>=50 & x<60) else x for x in df_test['AGE']]\n",
      "<ipython-input-10-d41cb0d64370>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['AGE_bin'] = [6 if x>=60 else x for x in df_test['AGE']]\n"
     ]
    }
   ],
   "source": [
    "df_train['AGE_bin'] = [0 if x<25 else x for x in df_train['AGE']]\n",
    "df_train['AGE_bin'] = [1 if (x>=25 & x<30) else x for x in df_train['AGE']]\n",
    "df_train['AGE_bin'] = [2 if (x>=30 & x<35) else x for x in df_train['AGE']]\n",
    "df_train['AGE_bin'] = [3 if (x>=35 & x<40) else x for x in df_train['AGE']]\n",
    "df_train['AGE_bin'] = [4 if (x>=40 & x<50) else x for x in df_train['AGE']]\n",
    "df_train['AGE_bin'] = [5 if (x>=50 & x<60) else x for x in df_train['AGE']]\n",
    "df_train['AGE_bin'] = [6 if x>=60 else x for x in df_train['AGE']]\n",
    "\n",
    "df_test['AGE_bin'] = [0 if x<25 else x for x in df_test['AGE']]\n",
    "df_test['AGE_bin'] = [1 if (x>=25 & x<30) else x for x in df_test['AGE']]\n",
    "df_test['AGE_bin'] = [2 if (x>=30 & x<35) else x for x in df_test['AGE']]\n",
    "df_test['AGE_bin'] = [3 if (x>=35 & x<40) else x for x in df_test['AGE']]\n",
    "df_test['AGE_bin'] = [4 if (x>=40 & x<50) else x for x in df_test['AGE']]\n",
    "df_test['AGE_bin'] = [5 if (x>=50 & x<60) else x for x in df_test['AGE']]\n",
    "df_test['AGE_bin'] = [6 if x>=60 else x for x in df_test['AGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'AGE_bin']]\n",
    "df_test = df_test[['id', 'AGE_bin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-09 11:21:16.070979\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## phone count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-09 11:21:25.117400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"phone_cnt_wrt_seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'MPHN_NO', 'PAYR_SEQ']]\n",
    "df_test = test_df[['id', 'PAYR_SEQ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = df_train[['MPHN_NO', 'PAYR_SEQ']].groupby(by=[\"PAYR_SEQ\"]).agg(['nunique']).reset_index()\n",
    "count_df.columns = [\"PAYR_SEQ\", \"phone_cnt_wrt_seq\"]\n",
    "df_train = df_train.merge(count_df, on=['PAYR_SEQ'], how=\"left\")[['id', 'phone_cnt_wrt_seq']]\n",
    "df_test = df_test.merge(count_df, on=['PAYR_SEQ'], how=\"left\")[['id', 'phone_cnt_wrt_seq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-09 11:21:42.805007\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-09 11:21:53.409736\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"seq_count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['id', 'PAYR_SEQ']]\n",
    "df_test = test_df[['id', 'PAYR_SEQ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = df_train[['id', 'PAYR_SEQ']].groupby(by=[\"PAYR_SEQ\"]).count().reset_index()\n",
    "count_df.columns = [\"PAYR_SEQ\", \"seq_count\"]\n",
    "df_train = df_train.merge(count_df, on=['PAYR_SEQ'], how=\"left\")[['id', 'seq_count']]\n",
    "df_test = df_test.merge(count_df, on=['PAYR_SEQ'], how=\"left\")[['id', 'seq_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-09 11:22:06.456821\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is_targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2020-11-09 11:50:09.976364\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starts at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"is_targeted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-10490379352a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['cumsum'] = df_train[['MPHN_NO', 'target']].groupby(['MPHN_NO'])['target'].cumsum()\n",
      "<ipython-input-17-10490379352a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['is_targeted'] = [0 if x<=1 else 1 for x in df_train['cumsum']]\n"
     ]
    }
   ],
   "source": [
    "df_train = train_df[['id', 'MPHN_NO', 'target']]\n",
    "df_train['cumsum'] = df_train[['MPHN_NO', 'target']].groupby(['MPHN_NO'])['target'].cumsum()\n",
    "df_train['is_targeted'] = [0 if x<=1 else 1 for x in df_train['cumsum']]\n",
    "\n",
    "is_targeted = df_train[['MPHN_NO', 'is_targeted']].drop_duplicates(subset=['MPHN_NO'], keep='last')\n",
    "df_test = test_df[['id', 'MPHN_NO']].merge(is_targeted, on=['MPHN_NO'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['id', 'is_targeted']]\n",
    "df_test = df_test[['id', 'is_targeted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(bagging_size):\n",
    "    with open(f\"{data_dir}indices_{seed}.pkl\", 'rb') as f:\n",
    "        indices = joblib.load(f)\n",
    "    df_train.iloc[indices].to_parquet(f\"{data_dir}{FILE_NAME}_{seed}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_parquet(f\"{data_dir}{FILE_NAME}_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ends at 2020-11-09 11:50:30.315963\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ends at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
